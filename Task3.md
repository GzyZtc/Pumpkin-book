<h1>决策树</h1>
<h2>定义</h2>

分类决策树是一种描述对实例进行分类的树形结构，决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶节点(leaf)。内部结点表示一个特征或属性，叶结点表示一个类。

<h2>决策树与if-then规则</h2>
可以将决策树看成一个if-then规则的集合。由决策树的根节点到叶结点的每一条路径构建一条规则：路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质:互斥且完备。每一个实例都被且只被一条路径或一条规则所覆盖

<h2>决策树与条件概率分布</h2>

<h2>决策树学习</h2>
假设给定数据集

$$\begin{equation*}
D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
\end{equation*}$$

其中, $x_i=(x_i^{(1)},...,x_i^{(n)})^T$ 为输入特征向量（实例），n为特征个数 $y_i \in \{ 1,2,...,K\}$ 类标记， $i=1,2,...,N$。决策树学习的目标是根据给定的训练数据构建一个决策树模型，使它能够正确的分类。
<br>
决策树学习用损失函数表示这一目标，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习策略是以损失函数为目标函数的最小化.当损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中的决策树多采用启发式算法，近似求解这一问题。这样得到的决策树是次最优的。
<br>
决策树学习的算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这一过程对应着对特征空间的划分，也对应着决策树的构建

<ol>
<li>构建根节点，将所有训练数据都放在根节点
<li>选取最优特征，按照这一特征将训练数据集分割成子集，使得各个子集分到所对应的叶结点中去
<li>如果子集已经能被正确分类，那么构建叶结点并将这些子集分到所对应的叶结点中去
<li>如果子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续分割，构建相应的结点
<li>如此递归下去，直到所有的训练数据子集被正确的分类，或者没有合适的特征为止
</ol>

最后每一个子集都被分到叶结点上，即都有了明确的类，这就生成了一棵决策树
<h2>特征选择</h2>
特征选择在于对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比.
<h3>信息增益</h3>
先给出熵与条件熵的定义<br>
在信息论与概率统计中，熵(entropy)是表示随机变量不确定性的度量。设X是一个有限个值的离散随机变量，其概率分布为

$$P(X=x_i)=p_i,\qquad i=1,2,.., n$$

则随机变量X的熵定义为

$$H(X)=-\overset{n}{\underset{i=1}{\sum}}p_ilogp_i$$

熵越大，随机变量的不确定性越大，从定义可以验证

$$0\leq H(p)\leq logn$$

当随机变量只取两个值，例如0,1，即X的分布为

$$P(X=1)=p,P(X=0)=1-p,\qquad 0\leq p \leq 1$$

熵为

$$H(p)=-plog_2p=(1-p)log_2(1-p)$$

当 $p=0$ 或 $p=1$ 时 $H(p)=0$，随机变量完全没有不确定性。当 $p=0.5$时 $H(p)=1$，熵取值最大，随机变量不确定性最大

<br>

设由随机变量 $(X,Y)$,其联合概率分布为

$$P(X=x_i,Y=y_i)=p_{i,j} \qquad i=1,2,...,n;\qquad j=1,2,...,m$$

